{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8224cfca-63cd-4397-8a7d-892f1b0577d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:00.831691Z",
     "start_time": "2025-03-19T14:30:00.488088Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "from scipy.stats import trim_mean\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from langdetect import detect, LangDetectException\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8bce70a1f5acf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:00.835903Z",
     "start_time": "2025-03-19T14:30:00.833928Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_runtime(df, k=0.1):\n",
    "    \"\"\"\n",
    "    Preprocesses the runtime information, including handling extreme values\n",
    "    (movies with runtime > 1000 minutes, possibly in seconds or hours).\n",
    "\n",
    "    Arguments:\n",
    "    - df: DataFrame containing the movie data.\n",
    "    - k: Proportion of values to trim from each end when computing the trimmed mean.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with cleaned runtime information.\n",
    "    \"\"\"\n",
    "    # First convert to numeric, coercing errors to NaN\n",
    "    df[\"runtimeMinutes\"] = pd.to_numeric(df[\"runtimeMinutes\"], errors='coerce')\n",
    "\n",
    "    # Compute k-trimmed mean for runtimeMinutes (ignoring NaN values)\n",
    "    trimmed_mean_runtime = trim_mean(df[\"runtimeMinutes\"].dropna().values, proportiontocut=k)\n",
    "\n",
    "    # Fill missing values with trimmed mean\n",
    "    df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].fillna(trimmed_mean_runtime)\n",
    "\n",
    "    # Handle movies that are possibly in seconds or hours\n",
    "    df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].apply(lambda x: x / 60 if pd.notna(x) and x > 1000 else x)  # Convert seconds to minutes\n",
    "    df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].apply(lambda x: x * 60 if pd.notna(x) and x < 5 else x)  # Convert minutes to hours if under 5 mins\n",
    "\n",
    "    # Now convert to int (after handling extreme values and filling NaNs)\n",
    "    df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].round().astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd754dbee92a8c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:00.918489Z",
     "start_time": "2025-03-19T14:30:00.916186Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_merge_imdb_data(data_path, directors_path, writers_path):\n",
    "    \"\"\"\n",
    "    Step 1: Load, merge, and perform initial cleaning of IMDB data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Step 2: Load JSON files (Directors & Writers)\n",
    "    df_directors = pd.read_json(directors_path)\n",
    "    df_writers = pd.read_json(writers_path)\n",
    "\n",
    "    # Step 3: Rename columns for consistency\n",
    "    df_directors.rename(columns={\"movie\": \"tconst\", \"director\": \"director_id\"}, inplace=True)\n",
    "    df_writers.rename(columns={\"movie\": \"tconst\", \"writer\": \"writer_id\"}, inplace=True)\n",
    "\n",
    "    # Step 4: Convert nested JSON fields into strings\n",
    "    df_directors[\"director_id\"] = df_directors[\"director_id\"].astype(str)\n",
    "    df_writers[\"writer_id\"] = df_writers[\"writer_id\"].astype(str)\n",
    "\n",
    "    # Step 5: Merge main dataset with Directors & Writers using DuckDB\n",
    "    con = duckdb.connect()\n",
    "    con.register(\"movies\", df)\n",
    "    con.register(\"directors\", df_directors)\n",
    "    con.register(\"writers\", df_writers)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        movies.*,\n",
    "        directors.director_id,\n",
    "        writers.writer_id\n",
    "    FROM movies\n",
    "    LEFT JOIN directors ON movies.tconst = directors.tconst\n",
    "    LEFT JOIN writers ON movies.tconst = writers.tconst\n",
    "    \"\"\"\n",
    "    df = con.execute(query).fetchdf()\n",
    "    con.close()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1588593-ab24-49a5-bb51-d755af920960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:00.926047Z",
     "start_time": "2025-03-19T14:30:00.921576Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_imdb_data(df):\n",
    "    \"\"\"\n",
    "    General preprocessing pipeline for IMDB data.\n",
    "    \n",
    "    Arguments:\n",
    "    - data_path: Path to the train/test/validation data CSV file.\n",
    "    - directors_path: Path to the directing.json file.\n",
    "    - writers_path: Path to the writing.json file.\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned Pandas DataFrame ready for model training or prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 6: Create column year from startYear and endYear\n",
    "    df['startYear'] = df['startYear'].replace('\\\\N', np.nan).astype(float)\n",
    "    df['endYear'] = df['endYear'].replace('\\\\N', np.nan).astype(float)\n",
    "    df['Year'] = df['startYear'].fillna(df['endYear'])\n",
    "\n",
    "    # Step 7: Add language detection\n",
    "    def detect_language(row):\n",
    "        try:\n",
    "            # Try to detect language from originalTitle first\n",
    "            if pd.notna(row['originalTitle']) and row['originalTitle'].strip():\n",
    "                return detect(row['originalTitle'])\n",
    "            # If originalTitle is missing, try primaryTitle\n",
    "            elif pd.notna(row['primaryTitle']) and row['primaryTitle'].strip():\n",
    "                language = detect(row['primaryTitle'])\n",
    "                # If primaryTitle is detected as English, return 'en'\n",
    "                return language\n",
    "            else:\n",
    "                return 'unknown'\n",
    "        except LangDetectException:\n",
    "            return 'unknown'\n",
    "\n",
    "    # Add language column before dropping originalTitle\n",
    "    df['title_language'] = df.apply(detect_language, axis=1)\n",
    "\n",
    "    # Step 8: Handle the language feature more robustly\n",
    "    df['title_language'] = df['title_language'].fillna('unknown')\n",
    "\n",
    "    # Step 9: Clean title names\n",
    "    def normalize_text(text):\n",
    "        if pd.isna(text):  # Handle missing values\n",
    "            return \"\"\n",
    "        text = str(text)\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')  # Remove accents\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "        return text.strip()\n",
    "\n",
    "    def clean_titles(row):\n",
    "        primary = row['primaryTitle'] if pd.notna(row['primaryTitle']) else ''\n",
    "        original = row['originalTitle'] if pd.notna(row['originalTitle']) else ''\n",
    "\n",
    "        if not primary:\n",
    "            primary = original\n",
    "\n",
    "        cleaned_title = normalize_text(primary)\n",
    "\n",
    "        return cleaned_title if cleaned_title else \"Unknown Title\"\n",
    "\n",
    "    df['primaryTitle'] = df.apply(clean_titles, axis=1)\n",
    "    df.rename(columns={'primaryTitle': 'movieTitle'}, inplace=True)\n",
    "\n",
    "    # Step 10: Compute Title Uniqueness Score\n",
    "    title_counts = df[\"movieTitle\"].value_counts()\n",
    "    df[\"title_uniqueness\"] = df[\"movieTitle\"].apply(lambda x: 1 / title_counts[x] if title_counts[x] > 1 else 1)\n",
    "\n",
    "    # Step 11: Compute Sentiment Score\n",
    "    df[\"sentiment_score\"] = df[\"movieTitle\"].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Step 12: Count words in each title\n",
    "    df[\"word_count\"] = df[\"movieTitle\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Step 13 title length\n",
    "    df[\"title_word_length_std\"] = df[\"movieTitle\"].apply(lambda x: np.std([len(word) for word in x.split()]) if len(x.split()) > 1 else 0)\n",
    "    \n",
    "    # Step 14: Drop unnecessary columns\n",
    "    columns_to_drop = [\"originalTitle\", \"endYear\", \"startYear\", \"Unnamed: 0\"]\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "    # Step 15: Handle missing values\n",
    "    df = preprocess_runtime(df, 0.1)\n",
    "\n",
    "    # Step 16: Fill missing values for numVotes\n",
    "    trimmed_mean_votes = trim_mean(df[\"numVotes\"].dropna(), proportiontocut=0.1)\n",
    "    df[\"numVotes\"] = df[\"numVotes\"].fillna(trimmed_mean_votes)\n",
    "\n",
    "\n",
    "    # Step 17: Fill missing values for director_id and writer_id\n",
    "    df[\"director_id\"] = df[\"director_id\"].fillna(\"unknown\")\n",
    "    df[\"writer_id\"] = df[\"writer_id\"].fillna(\"unknown\")\n",
    "\n",
    "    # Step 18: Ensure correct data types\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "    df[\"numVotes\"] = df[\"numVotes\"].astype(int)\n",
    "\n",
    "    # Step 19: Handle remaining categorical columns\n",
    "    categorical_features = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "    # Step 20: For any remaining categorical columns, apply label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Step 21: Ensure each `tconst` is unique\n",
    "    df = df.groupby(\"tconst\").first().reset_index()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b782cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.364803Z",
     "start_time": "2025-03-19T14:30:00.930944Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m df_intermediate = pd.concat([load_and_merge_imdb_data(file, directors_path, writers_path) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m train_files])\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Step 2: Process features and finalize dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df_train= \u001b[43mpreprocess_imdb_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_intermediate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# # Preprocess and merge all training data\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# df_train = pd.concat([preprocess_imdb_data(file, directors_path, writers_path) for file in train_files], ignore_index=True)\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Preprocess validation and test data\u001b[39;00m\n\u001b[32m     26\u001b[39m df_intermediate_val = load_and_merge_imdb_data(os.path.join(base_data_dir, \u001b[33m\"\u001b[39m\u001b[33mvalidation_hidden.csv\u001b[39m\u001b[33m\"\u001b[39m), directors_path, writers_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mpreprocess_imdb_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     66\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtitle_uniqueness\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mmovieTitle\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[32m1\u001b[39m / title_counts[x] \u001b[38;5;28;01mif\u001b[39;00m title_counts[x] > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Step 11: Compute Sentiment Score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmovieTitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msentiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolarity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Step 12: Count words in each title\u001b[39;00m\n\u001b[32m     72\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mword_count\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mmovieTitle\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x.split()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mpreprocess_imdb_data.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     66\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtitle_uniqueness\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mmovieTitle\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[32m1\u001b[39m / title_counts[x] \u001b[38;5;28;01mif\u001b[39;00m title_counts[x] > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Step 11: Compute Sentiment Score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mmovieTitle\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msentiment\u001b[49m.polarity)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Step 12: Count words in each title\u001b[39;00m\n\u001b[32m     72\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mword_count\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mmovieTitle\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x.split()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\textblob\\decorators.py:23\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\textblob\\blob.py:439\u001b[39m, in \u001b[36mBaseBlob.sentiment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msentiment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a tuple of form (polarity, subjectivity ) where polarity\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    is a float within the range [-1.0, 1.0] and subjectivity is a float\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[33;03m    within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    437\u001b[39m \u001b[33;03m    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)``\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\textblob\\en\\sentiments.py:44\u001b[39m, in \u001b[36mPatternAnalyzer.analyze\u001b[39m\u001b[34m(self, text, keep_assessments)\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Sentiment(polarity, subjectivity, assessments)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     Sentiment = \u001b[43mnamedtuple\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpolarity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubjectivity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Sentiment(*pattern_sentiment(text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\collections\\__init__.py:444\u001b[39m, in \u001b[36mnamedtuple\u001b[39m\u001b[34m(typename, field_names, rename, defaults, module)\u001b[39m\n\u001b[32m    438\u001b[39m namespace = {\n\u001b[32m    439\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_tuple_new\u001b[39m\u001b[33m'\u001b[39m: tuple_new,\n\u001b[32m    440\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m__builtins__\u001b[39m\u001b[33m'\u001b[39m: {},\n\u001b[32m    441\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamedtuple_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m    442\u001b[39m }\n\u001b[32m    443\u001b[39m code = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlambda _cls, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: _tuple_new(_cls, (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m))\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m \u001b[34m__new__\u001b[39m = \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[34m__new__\u001b[39m.\u001b[34m__name__\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m__new__\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[34m__new__\u001b[39m.\u001b[34m__doc__\u001b[39m = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCreate new instance of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "# Define the base directory\n",
    "base_data_dir = os.path.join(os.getcwd(), \"imdb\")\n",
    "\n",
    "# Generate the list of train file paths\n",
    "train_files = [os.path.join(base_data_dir, f) for f in os.listdir(base_data_dir) if f.startswith(\"train-\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Define paths for directors and writers files\n",
    "directors_path = os.path.join(base_data_dir, \"directing.json\")\n",
    "writers_path = os.path.join(base_data_dir, \"writing.json\")\n",
    "\n",
    "# Load JSON files (Directors & Writers)\n",
    "df_directors = pd.read_json(directors_path)\n",
    "df_writers = pd.read_json(writers_path)\n",
    "\n",
    "# Step 1: Load and merge raw data\n",
    "df_intermediate = pd.concat([load_and_merge_imdb_data(file, directors_path, writers_path) for file in train_files])\n",
    "\n",
    "# Step 2: Process features and finalize dataset\n",
    "df_train= preprocess_imdb_data(df_intermediate)\n",
    "\n",
    "# # Preprocess and merge all training data\n",
    "# df_train = pd.concat([preprocess_imdb_data(file, directors_path, writers_path) for file in train_files], ignore_index=True)\n",
    "\n",
    "# Preprocess validation and test data\n",
    "df_intermediate_val = load_and_merge_imdb_data(os.path.join(base_data_dir, \"validation_hidden.csv\"), directors_path, writers_path)\n",
    "df_val = preprocess_imdb_data(df_intermediate_val)\n",
    "df_intermediate_test= load_and_merge_imdb_data(os.path.join(base_data_dir, \"test_hidden.csv\"), directors_path, writers_path)\n",
    "df_test = preprocess_imdb_data(df_intermediate_test)\n",
    "\n",
    "print(\"NaN values at the end\", df_train.isna().sum())\n",
    "\n",
    "# Save cleaned datasets\n",
    "df_train.to_csv(\"cleaned/final_training_data_rottenMovies.csv\", index=False)\n",
    "df_val.to_csv(\"cleaned/final_validation_data_rottenMovies.csv\", index=False)\n",
    "df_test.to_csv(\"cleaned/final_test_data_rottenMovies.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ All datasets have been preprocessed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852683f7af4a0cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.376520Z",
     "start_time": "2025-03-19T14:30:52.373946Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"NaN values at the end\", df_val.isna().sum())\n",
    "print(\"NaN values at the end\", df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42f0eed525e31a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.391027Z",
     "start_time": "2025-03-19T14:30:52.385328Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_unique_ratio(df, columns=None):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of unique rows to total rows in the DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    - df: DataFrame to analyze\n",
    "    - columns: List of columns to consider (if None, uses all columns)\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing unique ratio metrics\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "\n",
    "    total_rows = len(df)\n",
    "    unique_rows = len(df[columns].drop_duplicates())\n",
    "    ratio = unique_rows / total_rows\n",
    "\n",
    "    metrics = {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"unique_rows\": unique_rows,\n",
    "        \"unique_ratio\": ratio\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Add this after your data preprocessing\n",
    "print(\"\\nüîç Analyzing unique row ratios...\")\n",
    "\n",
    "# Calculate ratios for all datasets\n",
    "train_metrics = calculate_unique_ratio(df_train)\n",
    "val_metrics = calculate_unique_ratio(df_val)\n",
    "test_metrics = calculate_unique_ratio(df_test)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nUnique Row Analysis:\")\n",
    "print(f\"Training Data:\")\n",
    "print(f\"  - Total Rows: {train_metrics['total_rows']:,}\")\n",
    "print(f\"  - Unique Rows: {train_metrics['unique_rows']:,}\")\n",
    "print(f\"  - Unique Ratio: {train_metrics['unique_ratio']:.2%}\")\n",
    "\n",
    "print(f\"\\nValidation Data:\")\n",
    "print(f\"  - Total Rows: {val_metrics['total_rows']:,}\")\n",
    "print(f\"  - Unique Rows: {val_metrics['unique_rows']:,}\")\n",
    "print(f\"  - Unique Ratio: {val_metrics['unique_ratio']:.2%}\")\n",
    "\n",
    "print(f\"\\nTest Data:\")\n",
    "print(f\"  - Total Rows: {test_metrics['total_rows']:,}\")\n",
    "print(f\"  - Unique Rows: {test_metrics['unique_rows']:,}\")\n",
    "print(f\"  - Unique Ratio: {test_metrics['unique_ratio']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e86c7b646b0ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.408498Z",
     "start_time": "2025-03-19T14:30:52.402716Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_duplicates(df, groupby_cols=None, agg_strategy=None):\n",
    "    \"\"\"\n",
    "    Handle duplicate rows using specified aggregation strategies.\n",
    "\n",
    "    Arguments:\n",
    "    - df: DataFrame to process\n",
    "    - groupby_cols: List of columns to identify duplicates (default: all columns except label)\n",
    "    - agg_strategy: Dictionary of column names and aggregation functions\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with handled duplicates\n",
    "    \"\"\"\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = [col for col in df.columns if col != 'label']\n",
    "\n",
    "    if agg_strategy is None:\n",
    "        agg_strategy = {\n",
    "            'runtimeMinutes': 'mean',\n",
    "            'numVotes': 'sum',\n",
    "            'startYear': 'first',\n",
    "            'director_id': 'first',\n",
    "            'writer_id': 'first',\n",
    "            'label': 'mode'\n",
    "        }\n",
    "\n",
    "    # Count occurrences before deduplication\n",
    "    total_rows = len(df)\n",
    "    unique_rows = len(df[groupby_cols].drop_duplicates())\n",
    "\n",
    "    if total_rows == unique_rows:\n",
    "        print(\"No duplicates found!\")\n",
    "        return df\n",
    "\n",
    "    print(f\"\\nFound {total_rows - unique_rows:,} duplicate rows\")\n",
    "    print(f\"Unique ratio before: {(unique_rows/total_rows):.2%}\")\n",
    "\n",
    "    # Handle duplicates using aggregation\n",
    "    df_cleaned = df.groupby(groupby_cols, as_index=False).agg(agg_strategy)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Rows after duplicate handling: {len(df_cleaned):,}\")\n",
    "    print(f\"Unique ratio after: {(len(df_cleaned)/total_rows):.2%}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# Handle duplicates\n",
    "print(\"\\nüîç Handling duplicates...\")\n",
    "df_train = handle_duplicates(df_train)\n",
    "df_val = handle_duplicates(df_val)\n",
    "df_test = handle_duplicates(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f01c3fedfa7461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.670236Z",
     "start_time": "2025-03-19T14:30:52.417775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load preprocessed training dataset\n",
    "df_train = pd.read_csv(\"cleaned/final_training_data_titlefeatures.csv\")\n",
    "\n",
    "print(\"NaN values at the end\", df_train.isna().sum())\n",
    "\n",
    "# Boxplots to check for outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df_train)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57f9b5b6f30438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.732611Z",
     "start_time": "2025-03-19T14:30:52.679006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the distribution of labels\n",
    "label_counts = df_train['label'].value_counts()\n",
    "print(f\"Label distribution:\\n{label_counts}\")\n",
    "print(f\"Percentage of positive labels: {label_counts[True]/len(df_train)*100:.2f}%\")\n",
    "print(f\"Percentage of negative labels: {label_counts[False]/len(df_train)*100:.2f}%\")\n",
    "\n",
    "# Create a bar plot to visualize the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='label', data=df_train, hue='label', palette=['plum', 'blue'])\n",
    "plt.title('Distribution of Target Labels')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Not Critically Acclaimed (False)', 'Critically Acclaimed (True)'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if dataset is balanced\n",
    "if abs(label_counts[False] - label_counts[True])/len(df_train) < 0.1:\n",
    "    print(\"‚úÖ Dataset is roughly balanced\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset is imbalanced - consider using class weights or resampling techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c25ca6-e414-426c-9b66-901333d1d979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.744922Z",
     "start_time": "2025-03-19T14:30:52.742629Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load preprocessed training dataset\n",
    "# df_train = pd.read_csv(\"cleaned/final_training_data_titlefeatures.csv\")\n",
    "#\n",
    "# # Apply frequency encoding to categorical variables\n",
    "# for col in [\"director_id\", \"writer_id\"]:\n",
    "#     freq_encoding = df_train[col].value_counts(normalize=True)\n",
    "#     df_train[col] = df_train[col].map(freq_encoding)\n",
    "#\n",
    "# # Define Features & Target\n",
    "# features = [\"Year\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\", \"word_count\", \"title_uniqueness\", \"title_word_length_std\", \"sentiment_score\"]\n",
    "# X = df_train[features]\n",
    "# y = df_train[\"label\"]  # Only train data has labels\n",
    "#\n",
    "# # **NEW: Split training data into train (80%) and validation (20%)**\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Preprocessing Pipeline (same for all models)\n",
    "# numeric_features = [\"Year\", \"runtimeMinutes\", \"numVotes\", \"word_count\", \"title_uniqueness\", \"title_word_length_std\", \"sentiment_score\"]\n",
    "#\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", StandardScaler(), numeric_features),\n",
    "#     ]\n",
    "# )\n",
    "# # Create pipeline with preprocessing\n",
    "# pipeline = Pipeline([\n",
    "#     (\"preprocessing\", preprocessor),\n",
    "#     (\"classifier\", SVC(kernel=\"linear\", probability=True))\n",
    "# ])\n",
    "#\n",
    "# # Train model\n",
    "# pipeline.fit(X_train, y_train)\n",
    "#\n",
    "# # Make predictions\n",
    "# y_train_pred = pipeline.predict(X_train)\n",
    "# y_val_pred = pipeline.predict(X_val)\n",
    "#\n",
    "# # Evaluate model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "#\n",
    "# print(f\"‚úÖ Training Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"‚úÖ Validation Accuracy: {val_accuracy:.4f}\")\n",
    "# print(f\"üìä Classification Report for:\\n\", classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09d2944571957d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.761530Z",
     "start_time": "2025-03-19T14:30:52.753559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load preprocessed training dataset\n",
    "df_train = pd.read_csv(\"cleaned/final_training_data_titlefeatures.csv\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62bd826dd7e690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:30:52.846384Z",
     "start_time": "2025-03-19T14:30:52.844296Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"NaN values at the end\", df_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02ac3f-4944-4f00-ad28-823e85a97f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:31:22.157048Z",
     "start_time": "2025-03-19T14:30:52.866557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load preprocessed training dataset\n",
    "df_train = pd.read_csv(\"cleaned/final_training_data_titlefeatures.csv\")\n",
    "\n",
    "# Apply frequency encoding to categorical variables\n",
    "for col in [\"director_id\", \"writer_id\"]:\n",
    "    freq_encoding = df_train[col].value_counts(normalize=True)\n",
    "    df_train[col] = df_train[col].map(freq_encoding)\n",
    "\n",
    "X = df_train.drop(columns = [\"label\"])\n",
    "y = df_train[\"label\"]  # Only train data has labels\n",
    "\n",
    "# Split training data into train (80%) and validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing Pipeline for all the numerical features\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features ),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create base pipeline with preprocessing\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", None)  # Placeholder for classifier\n",
    "])\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"classifier\": [LogisticRegression(max_iter=1000, random_state=42)],\n",
    "        \"classifier__C\": [0.1, 1.0, 10.0],\n",
    "        \"classifier__solver\": [\"liblinear\", \"lbfgs\"]\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"classifier\": [SVC(probability=True, random_state=42)],\n",
    "        \"classifier__C\": [0.1, 1.0, 10.0],\n",
    "        \"classifier__kernel\": [\"linear\", \"rbf\"]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"classifier\": [RandomForestClassifier(random_state=42)],\n",
    "        \"classifier__n_estimators\": [100, 200],\n",
    "        \"classifier__max_depth\": [None, 20]\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"classifier\": [GradientBoostingClassifier(random_state=42)],\n",
    "        \"classifier__n_estimators\": [100, 200],\n",
    "        \"classifier__learning_rate\": [0.01, 0.1]\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"classifier\": [AdaBoostClassifier(random_state=42)],\n",
    "        \"classifier__n_estimators\": [50, 100, 200],\n",
    "        \"classifier__learning_rate\": [0.01, 0.1, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearch for each classifier\n",
    "results = {}\n",
    "print(\"üîç Starting grid search across classifiers...\")\n",
    "\n",
    "for name, param_grid in classifiers.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"best_model\": best_model,\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"best_cv_score\": grid.best_score_,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"time\": time.time() - start_time\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Best parameters: {grid.best_params_}\")\n",
    "    print(f\"‚úÖ Cross-validation accuracy: {grid.best_score_:.4f}\")\n",
    "    print(f\"‚úÖ Validation accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"‚è±Ô∏è Time: {results[name]['time']:.2f} seconds\")\n",
    "\n",
    "# Print summary table sorted by validation accuracy\n",
    "print(\"\\nüìä Summary of Results (sorted by validation accuracy):\")\n",
    "print(f\"{'Classifier':<20} {'Val Accuracy':<15} {'CV Accuracy':<15} {'Time (s)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, result in sorted(results.items(), key=lambda x: x[1][\"val_accuracy\"], reverse=True):\n",
    "    print(f\"{name:<20} {result['val_accuracy']:.4f}{' '*10} {result['best_cv_score']:.4f}{' '*10} {result['time']:.2f}\")\n",
    "\n",
    "# Get best model\n",
    "best_classifier = max(results.items(), key=lambda x: x[1][\"val_accuracy\"])\n",
    "best_name = best_classifier[0]\n",
    "best_result = best_classifier[1]\n",
    "best_model = best_result[\"best_model\"]\n",
    "\n",
    "print(f\"\\nüèÜ Best classifier: {best_name}\")\n",
    "print(f\"‚úÖ Validation accuracy: {best_result['val_accuracy']:.4f}\")\n",
    "\n",
    "# Detailed evaluation of best model\n",
    "y_val_pred = best_result[\"best_model\"].predict(X_val)\n",
    "print(f\"\\nüìä Classification Report for {best_name}:\\n\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce125a292ffb281f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:31:22.860579Z",
     "start_time": "2025-03-19T14:31:22.161110Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame(\n",
    "    {'feature': X_train.columns, 'importance': rf_selector.feature_importances_}\n",
    ")\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances)\n",
    "plt.title('Top Feature Importances')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('feature_importances.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# Select top features\n",
    "top_features = feature_importances['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a5609d456d8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:31:23.664269Z",
     "start_time": "2025-03-19T14:31:22.887114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Features & Target\n",
    "features = [\"Year\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\", \"word_count\"]\n",
    "X_train = df_train.drop(columns = [\"label\"])\n",
    "y_train = df_train[\"label\"]\n",
    "X_val = df_val\n",
    "X_test = df_test\n",
    "\n",
    "# Extract best classifier and its parameters from grid search\n",
    "best_classifier_step = best_model.named_steps['classifier']\n",
    "print(f\"Using {type(best_classifier_step).__name__} for final predictions\")\n",
    "\n",
    "# Preprocessing Pipeline for all the numerical features\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "final_model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", best_classifier_step)\n",
    "])\n",
    "\n",
    "print(\"üîπ Training model on full training data...\")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate Predictions\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Save predictions in required format (no headers, single column)\n",
    "pd.DataFrame(y_val_pred).to_csv(\"submissions/validation_predictions_best_model.csv\", index=False, header=False)\n",
    "pd.DataFrame(y_test_pred).to_csv(\"submissions/test_predictions_best_model.csv\", index=False, header=False)\n",
    "\n",
    "print(\"‚úÖ Predictions saved for submission!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7f2f0-168c-41c3-a748-b4d528ca4e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fe68a-86ae-4228-b0d8-38a870b07ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602da2f-f8c1-4cc4-a660-bc1bb2a51c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66c434-4569-4596-a059-3b47576fb551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b526bc2-7297-4fb5-89d9-1dc4f7bc64f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
