{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8224cfca-63cd-4397-8a7d-892f1b0577d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1588593-ab24-49a5-bb51-d755af920960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb_data(data_path, directors_path, writers_path):\n",
    "    \"\"\"\n",
    "    General preprocessing pipeline for IMDB data.\n",
    "    \n",
    "    Arguments:\n",
    "    - data_path: Path to the train/test/validation data CSV file.\n",
    "    - directors_path: Path to the directing.json file.\n",
    "    - writers_path: Path to the writing.json file.\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned Pandas DataFrame ready for model training or prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load main dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Step 2: Load JSON files (Directors & Writers)\n",
    "    df_directors = pd.read_json(directors_path)\n",
    "    df_writers = pd.read_json(writers_path)\n",
    "\n",
    "    # Step 3: Rename columns for consistency\n",
    "    df_directors.rename(columns={\"movie\": \"tconst\", \"director\": \"director_id\"}, inplace=True)\n",
    "    df_writers.rename(columns={\"movie\": \"tconst\", \"writer\": \"writer_id\"}, inplace=True)\n",
    "\n",
    "    # Step 4: Convert nested JSON fields into strings (fixes 'unhashable type' error)\n",
    "    df_directors[\"director_id\"] = df_directors[\"director_id\"].astype(str)\n",
    "    df_writers[\"writer_id\"] = df_writers[\"writer_id\"].astype(str)\n",
    "\n",
    "    # Step 5: Merge main dataset with Directors & Writers using DuckDB\n",
    "    con = duckdb.connect()\n",
    "    con.register(\"movies\", df)\n",
    "    con.register(\"directors\", df_directors)\n",
    "    con.register(\"writers\", df_writers)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        movies.*, \n",
    "        directors.director_id, \n",
    "        writers.writer_id\n",
    "    FROM movies\n",
    "    LEFT JOIN directors ON movies.tconst = directors.tconst\n",
    "    LEFT JOIN writers ON movies.tconst = writers.tconst\n",
    "    \"\"\"\n",
    "\n",
    "    df = con.execute(query).fetchdf()\n",
    "    con.close()\n",
    "\n",
    "    # Step 6: Drop unnecessary columns\n",
    "    columns_to_drop = [\"originalTitle\", \"endYear\", \"Unnamed: 0\"]\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "    # Step 7: Handle missing values\n",
    "    numeric_columns = [\"startYear\", \"runtimeMinutes\", \"numVotes\"]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")  # Ensure numeric format\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())  # Fill missing with median\n",
    "\n",
    "    df[\"director_id\"] = df[\"director_id\"].fillna(\"unknown\")\n",
    "    df[\"writer_id\"] = df[\"writer_id\"].fillna(\"unknown\")\n",
    "\n",
    "    # Step 8: Ensure correct data types\n",
    "    df[\"startYear\"] = df[\"startYear\"].astype(int)\n",
    "    df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].astype(int)\n",
    "    df[\"numVotes\"] = df[\"numVotes\"].astype(int)\n",
    "\n",
    "    # Step 9: Ensure each `tconst` is unique\n",
    "    df = df.groupby(\"tconst\").first().reset_index()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2b6b913-5ff5-44e7-bf55-8c9005c157dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All datasets have been preprocessed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "train_files = [\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-1.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-2.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-3.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-4.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-5.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-6.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-7.csv\",\n",
    "    \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-8.csv\"\n",
    "]\n",
    "\n",
    "directors_path = \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/directing.json\"\n",
    "writers_path = \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/writing.json\"\n",
    "\n",
    "# Preprocess and merge all training data\n",
    "df_train = pd.concat([preprocess_imdb_data(file, directors_path, writers_path) for file in train_files], ignore_index=True)\n",
    "\n",
    "# Preprocess validation and test data\n",
    "df_val = preprocess_imdb_data(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/validation_hidden.csv\", directors_path, writers_path)\n",
    "df_test = preprocess_imdb_data(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/test_hidden.csv\", directors_path, writers_path)\n",
    "\n",
    "# Save cleaned datasets\n",
    "df_train.to_csv(\"final_training_data.csv\", index=False)\n",
    "df_val.to_csv(\"final_validation_data.csv\", index=False)\n",
    "df_test.to_csv(\"final_test_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… All datasets have been preprocessed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a4931c9-d175-44ed-8e4d-2963d66be56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Training model on full training data...\n",
      "âœ… Predictions saved for submission!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define Features & Target\n",
    "features = [\"startYear\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\"]\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[\"label\"]\n",
    "X_val = df_val[features]\n",
    "X_test = df_test[features]\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_features = [\"startYear\", \"runtimeMinutes\", \"numVotes\"]\n",
    "categorical_features = [\"director_id\", \"writer_id\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "print(\"ðŸ”¹ Training model on full training data...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate Predictions\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions in required format (no headers, single column)\n",
    "pd.DataFrame(y_val_pred).to_csv(\"validation_predictions.csv\", index=False, header=False)\n",
    "pd.DataFrame(y_test_pred).to_csv(\"test_predictions.csv\", index=False, header=False)\n",
    "\n",
    "print(\"âœ… Predictions saved for submission!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742118be-6c14-4034-b74d-6ae7296dcec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b48c0-82cd-4535-9c95-9f27d6c9c802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835481e-6291-417f-a3a8-8752d053f59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
