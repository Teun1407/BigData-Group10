{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8224cfca-63cd-4397-8a7d-892f1b0577d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import duckdb\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm \n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e43172-53a1-48f2-a000-793f8b82c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb_data(data_path, directors_path, writers_path):\n",
    "    \"\"\"\n",
    "    General preprocessing pipeline for IMDB data with batch processing for BERT embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "    - data_path: Path to the train/test/validation data CSV file.\n",
    "    - directors_path: Path to the directing.json file.\n",
    "    - writers_path: Path to the writing.json file.\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned Pandas DataFrame ready for model training or prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load main dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Step 2: Load JSON files (Directors & Writers)\n",
    "    df_directors = pd.read_json(directors_path)\n",
    "    df_writers = pd.read_json(writers_path)\n",
    "\n",
    "    # Step 3: Rename columns for consistency\n",
    "    df_directors.rename(columns={\"movie\": \"tconst\", \"director\": \"director_id\"}, inplace=True)\n",
    "    df_writers.rename(columns={\"movie\": \"tconst\", \"writer\": \"writer_id\"}, inplace=True)\n",
    "\n",
    "    # Step 4: Convert nested JSON fields into strings (fixes 'unhashable type' error)\n",
    "    df_directors[\"director_id\"] = df_directors[\"director_id\"].astype(str)\n",
    "    df_writers[\"writer_id\"] = df_writers[\"writer_id\"].astype(str)\n",
    "\n",
    "    # Step 5: Merge main dataset with Directors & Writers using DuckDB\n",
    "    import duckdb\n",
    "    con = duckdb.connect()\n",
    "    con.register(\"movies\", df)\n",
    "    con.register(\"directors\", df_directors)\n",
    "    con.register(\"writers\", df_writers)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        movies.*, \n",
    "        directors.director_id, \n",
    "        writers.writer_id\n",
    "    FROM movies\n",
    "    LEFT JOIN directors ON movies.tconst = directors.tconst\n",
    "    LEFT JOIN writers ON movies.tconst = writers.tconst\n",
    "    \"\"\"\n",
    "\n",
    "    df = con.execute(query).fetchdf()\n",
    "    con.close()\n",
    "\n",
    "        # Step 6: Create column year from startYear and endYear\n",
    "    df['startYear'] = df['startYear'].replace('\\\\N', np.nan).astype(float)\n",
    "    df['endYear'] = df['endYear'].replace('\\\\N', np.nan).astype(float)\n",
    "    df['Year'] = df['startYear'].fillna(df['endYear'])\n",
    "\n",
    "    # Step 7: Clean title names\n",
    "    def normalize_text(text):\n",
    "        if pd.isna(text):  # Handle missing values\n",
    "            return \"\"\n",
    "        text = str(text)\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')  # Remove accents\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove any remaining special characters\n",
    "        return text.strip()\n",
    "\n",
    "    def clean_titles(row):\n",
    "        primary = row['primaryTitle'] if pd.notna(row['primaryTitle']) else ''\n",
    "        original = row['originalTitle'] if pd.notna(row['originalTitle']) else ''\n",
    "\n",
    "        # If primaryTitle is empty, replace it with originalTitle\n",
    "        if not primary:\n",
    "            primary = original\n",
    "\n",
    "        # Normalize primaryTitle\n",
    "        cleaned_title = normalize_text(primary)\n",
    "\n",
    "        # If both titles are missing after normalization, return \"Unknown Title\"\n",
    "        return cleaned_title if cleaned_title else \"Unknown Title\"\n",
    "\n",
    "    df['primaryTitle'] = df.apply(clean_titles, axis=1)\n",
    "    df.rename(columns={'primaryTitle': 'movieTitle'}, inplace=True)\n",
    "\n",
    "    # Step 8: Drop unnecessary columns\n",
    "    columns_to_drop = [\"startYear\", \"endYear\", \"originalTitle\", \"Unnamed: 0\"]\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "    # Step 9: Handle missing values\n",
    "    numeric_columns = [\"runtimeMinutes\", \"numVotes\"]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")  # Ensure numeric format\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())  # Fill missing with median\n",
    "\n",
    "    df[\"director_id\"] = df[\"director_id\"].fillna(\"unknown\")\n",
    "    df[\"writer_id\"] = df[\"writer_id\"].fillna(\"unknown\")\n",
    "\n",
    "    # Step 10: Ensure correct data types\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "    df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].astype(int)\n",
    "    df[\"numVotes\"] = df[\"numVotes\"].astype(int)\n",
    "    df[\"movieTitle\"] = df[\"movieTitle\"].astype(str)\n",
    "\n",
    "    # Step 11: Ensure each `tconst` is unique\n",
    "    df = df.groupby(\"tconst\").first().reset_index()\n",
    "\n",
    "    # Step 12: Count words in each title\n",
    "    df[\"word_count\"] = df[\"movieTitle\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7330371b-8199-4eb0-a423-b3da6dce353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All datasets have been preprocessed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "# Define the base directory\n",
    "base_data_dir = os.path.join(os.getcwd(), \"imdb\")\n",
    "\n",
    "# Generate the list of train file paths\n",
    "train_files = [os.path.join(base_data_dir, f) for f in os.listdir(base_data_dir) if f.startswith(\"train-\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Define paths for directors and writers files\n",
    "directors_path = os.path.join(base_data_dir, \"directing.json\")\n",
    "writers_path = os.path.join(base_data_dir, \"writing.json\")\n",
    "\n",
    "# Load JSON files (Directors & Writers)\n",
    "df_directors = pd.read_json(directors_path)\n",
    "df_writers = pd.read_json(writers_path)\n",
    "\n",
    "# Preprocess and merge all training data\n",
    "df_train = pd.concat([preprocess_imdb_data(file, directors_path, writers_path) for file in train_files], ignore_index=True)\n",
    "\n",
    "# Preprocess validation and test data\n",
    "df_val = preprocess_imdb_data(os.path.join(base_data_dir, \"validation_hidden.csv\"), directors_path, writers_path)\n",
    "df_test = preprocess_imdb_data(os.path.join(base_data_dir, \"test_hidden.csv\"), directors_path, writers_path)\n",
    "\n",
    "# Save cleaned datasets\n",
    "df_train.to_csv(\"cleaned/final_training_dataTitleFeatures.csv\", index=False)\n",
    "df_val.to_csv(\"cleaned/final_validation_dataTitleFeatures.csv\", index=False)\n",
    "df_test.to_csv(\"cleaned/final_test_dataTitle.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… All datasets have been preprocessed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2697d2f8-a404-465c-bb50-ceaae59c61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing features in X_train_small: ['Year', 'runtimeMinutes', 'numVotes', 'director_id', 'writer_id', 'word_count']\n",
      "ðŸ”¹ Training model on 80% of train data...\n",
      "\n",
      "ðŸ“Š Model Evaluation on Train Data Split (80/20)\n",
      "âœ… Accuracy: 0.5496\n",
      "âœ… Precision: 0.5287\n",
      "âœ… Recall: 0.9336\n",
      "âœ… F1 Score: 0.6751\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split df_train into train and validation (80/20 split)\n",
    "df_train_small, df_val_small = train_test_split(df_train, test_size=0.2, random_state=42, stratify=df_train[\"label\"])\n",
    "\n",
    "# Define Features (NO BERT EMBEDDINGS, ONLY STRUCTURED DATA)\n",
    "features = [\"Year\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\", \"word_count\"]\n",
    "X_train_small = df_train_small[features]\n",
    "X_val_small = df_val_small[features]\n",
    "y_train_small = df_train_small[\"label\"]\n",
    "y_val_small = df_val_small[\"label\"]\n",
    "\n",
    "# âœ… Check if all features exist\n",
    "print(\"Existing features in X_train_small:\", X_train_small.columns.tolist())\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_features = [\"Year\", \"runtimeMinutes\", \"numVotes\"]\n",
    "categorical_features = [\"director_id\", \"writer_id\"]\n",
    "\n",
    "# âœ… Fix: Handle unknown categories in `OrdinalEncoder`\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), categorical_features)  # âœ… Fix applied\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train RandomForest Model\n",
    "model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"ðŸ”¹ Training model on 80% of train data...\")\n",
    "model.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Generate Predictions on Validation Set\n",
    "y_val_pred = model.predict(X_val_small)\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "accuracy = accuracy_score(y_val_small, y_val_pred)\n",
    "precision = precision_score(y_val_small, y_val_pred, average=\"binary\")\n",
    "recall = recall_score(y_val_small, y_val_pred, average=\"binary\")\n",
    "f1 = f1_score(y_val_small, y_val_pred, average=\"binary\")\n",
    "\n",
    "# Print Results\n",
    "print(\"\\nðŸ“Š Model Evaluation on Train Data Split (80/20)\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b6b913-5ff5-44e7-bf55-8c9005c157dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  2.88it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.92it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:07<00:00,  3.86it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:14<00:00,  2.24it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:08<00:00,  3.73it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.84it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:08<00:00,  3.70it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.66it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:08<00:00,  3.68it/s]\n",
      "Processing DistilBERT Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All datasets have been preprocessed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "# Define the base directory\n",
    "base_data_dir = os.path.join(os.getcwd(), \"imdb\")\n",
    "\n",
    "# Generate the list of train file paths\n",
    "train_files = [os.path.join(base_data_dir, f) for f in os.listdir(base_data_dir) if f.startswith(\"train-\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Define paths for directors and writers files\n",
    "directors_path = os.path.join(base_data_dir, \"directing.json\")\n",
    "writers_path = os.path.join(base_data_dir, \"writing.json\")\n",
    "\n",
    "# Load JSON files (Directors & Writers)\n",
    "df_directors = pd.read_json(directors_path)\n",
    "df_writers = pd.read_json(writers_path)\n",
    "\n",
    "# Preprocess and merge all training data\n",
    "df_train = pd.concat([preprocess_imdb_data(file, directors_path, writers_path) for file in train_files], ignore_index=True)\n",
    "\n",
    "# Preprocess validation and test data\n",
    "df_val = preprocess_imdb_data(os.path.join(base_data_dir, \"validation_hidden.csv\"), directors_path, writers_path)\n",
    "df_test = preprocess_imdb_data(os.path.join(base_data_dir, \"test_hidden.csv\"), directors_path, writers_path)\n",
    "\n",
    "# Save cleaned datasets\n",
    "df_train.to_csv(\"cleaned/final_training_dataTitleFeatures.csv\", index=False)\n",
    "df_val.to_csv(\"cleaned/final_validation_dataTitleFeatures.csv\", index=False)\n",
    "df_test.to_csv(\"cleaned/final_test_dataTitle.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… All datasets have been preprocessed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a4931c9-d175-44ed-8e4d-2963d66be56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Training model on full training data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['nm0760196', 'nm0770234', 'nm2493041', 'nm0056030', 'nm0827854', 'nm0475852', 'nm0186091', 'nm0001692', 'nm0797869', 'nm0885108', 'nm0000578', 'nm0272568', 'nm1957032', 'nm0242757', 'nm1260607', 'nm0464180', 'nm2797828', 'nm1223294', 'nm0605266', 'nm0587518', 'nm1624755', 'nm0473329', 'nm1061993', 'nm1000159', 'nm0569891', 'nm2669989', 'nm3306459', 'nm5107883', 'nm6016411', 'nm0431172', 'nm0079387', 'nm1811747', 'nm0718469', 'nm0648740', 'nm0110564', 'nm0533666', 'nm0998110', 'nm0639858', 'nm0423646', 'nm1048540', 'nm8381668', 'nm0522028', 'nm3201804', 'nm4033941', 'nm4980768', 'nm0941262', 'nm1242277', 'nm3155648', 'nm0005197', 'nm0294457', 'nm0801885', 'nm1868917', 'nm1473037', 'nm1936947', 'nm1361273', 'nm1804614', 'nm0762517', 'nm1970598', 'nm0103971', 'nm0508998', 'nm2655364', 'nm0526019', 'nm0446819', 'nm0001774', 'nm0672769', 'nm2258148', 'nm0899121', 'nm0075263', 'nm0394233', 'nm0411259', 'nm3052903', 'nm6174515', 'nm4909815', 'nm3184677', 'nm2746751', 'nm0438089', 'nm0442646', 'nm0648464', 'nm0002225', 'nm1637832', 'nm0495898', 'nm0960253', 'nm0815044', 'nm0607422', 'nm0213726', 'nm1025280', 'nm5068576', 'nm3226283', 'nm2751807', 'nm0112409', 'nm2165987', 'nm0769874', 'nm1427149', 'nm0310597', 'nm1207166', 'nm0453579', 'nm7978876', 'nm0440913', 'nm0744107', 'nm0043214', 'nm0395105', 'nm0688361', 'nm3407375', 'nm0934863', 'nm0744985', 'nm0084114', 'nm1240647', 'nm0542752', 'nm0924429', 'nm0462277', 'nm0845618', 'nm0289800', 'nm0330456', 'nm0389514', 'nm1927689', 'nm0449868', 'nm0653768', 'nm0453115', 'nm0905592', 'nm0586841', 'nm0100522', 'nm0286790', 'nm0699095', 'nm2988421', 'nm4171068', 'nm0602452', 'nm2473806', 'nm1912285', 'nm0660453', 'nm0868871', 'nm0000045', 'nm6512883', 'nm0040575', 'nm1738698', 'nm2269581', 'nm3818919', 'nm1583960', 'nm0373154', 'nm0115269', 'nm4335588', 'nm0013972', 'nm0950620', 'nm1491597', 'nm10470070', 'nm0712444', 'nm3147478', 'nm0909429', 'nm0074723', 'nm1028575', 'nm8784283', 'nm0518363', 'nm0001408', 'nm0732106', 'nm0774143', 'nm0698781', 'nm0076779', 'nm0925482', 'nm0491522', 'nm0775097', 'nm1733624', 'nm0780643', 'nm0559564', 'nm1497931', 'nm0522604', 'nm1630654', 'nm0499215', 'nm7355036', 'nm0511393', 'nm1644484', 'nm8399396', 'nm1104118', 'nm0807312', 'nm1009775', 'nm0000104', 'nm0001471', 'nm0106928', 'nm0381450', 'nm0491708', 'nm0665163', 'nm0425894', 'nm0381478', 'nm1154184', 'nm4756103', 'nm0236226', 'nm1466818', 'nm1913201', 'nm0379797', 'nm1146668', 'nm0164999', 'nm0366004', 'nm0725345', 'nm0629580', 'nm0006639', 'nm0869857', 'nm0953216', 'nm1215448', 'nm4183075', 'nm1190532', 'nm2171755', 'nm0466349', 'nm1154600', 'nm0281869', 'nm1783265', 'nm0672704', 'nm1079001', 'nm0071611', 'nm5832876', 'nm0690677', 'nm0618405', 'nm0641007', 'nm0397565', 'nm0826814', 'nm1317258', 'nm0210218', 'nm0523154', 'nm0427037', 'nm2719270', 'nm0364252', 'nm0945412', 'nm2264935', 'nm1027891', 'nm0786919', 'nm1062423', 'nm0014960', 'nm2403746', 'nm0215085', 'nm0217803', 'nm0089176', 'nm0504641', 'nm6304969', 'nm0064593', 'nm1206844', 'nm0056527', 'nm0359081', 'nm0013288', 'nm1119099', 'nm0139111', 'nm0836715', 'nm0350455', 'nm1577772', 'nm0562266', 'nm0258418', 'nm0317411', 'nm3640095', 'nm2692858', 'nm0504654', 'nm2165634', 'nm7618090', 'nm1145617', 'nm0575389', 'nm1279573', 'nm1293863', 'nm0382779', 'nm1077277', 'nm1561205', 'nm0892400', 'nm0017004', 'nm0906476', 'nm4364444', 'nm2325602', 'nm0406687', 'nm0229694', 'nm0162979', 'nm8011325', 'nm0468882', 'nm1231757', 'nm0839268', 'nm0918041', 'nm0086144', 'nm0580727', 'nm2975950', 'nm0049335', 'nm1362893', 'nm1840195', 'nm0568478', 'nm2348718', 'nm0245361', 'nm0187834', 'nm0769703', 'nm4032740', 'nm1392439', 'nm0954500', 'nm5824384', 'nm1262599', 'nm7916459', 'nm0168892', 'nm3255161', 'nm0779011', 'nm0514816', 'nm0170504', 'nm3616958', 'nm0683578', 'nm0196930', 'nm0233050', 'nm1558526', 'nm0412220', 'nm0009053', 'nm1371451', 'nm1685252', 'nm1204790', 'nm0030612', 'nm0340244', 'nm9214307', 'nm0404606', 'nm0178876', 'nm7552433', 'nm0352524', 'nm5617646', 'nm0663489', 'nm0373612', 'nm0063581', 'nm0916859', 'nm0836708', 'nm0006939', 'nm2395339', 'nm0661919', 'nm2128335', 'nm4298518', 'nm0001490', 'nm0481195', 'nm3936005', 'nm0768959', 'nm1396593', 'nm0773108', 'nm0292134', 'nm0645661', 'nm3192191', 'nm0344144', 'nm6921776', 'nm1902132', 'nm0848992', 'nm1220246', 'nm2085752', 'nm0228013', 'nm1434014', 'nm0915304', 'nm0622288', 'nm1259604', 'nm0861899', 'nm0025399', 'nm0465551', 'nm1026530', 'nm0734466', 'nm0446059', 'nm0770647', 'nm0006853', 'nm0566671', 'nm2141317', 'nm0493615', 'nm0051156', 'nm0262402', 'nm0561938', 'nm0046480', 'nm0009942', 'nm3010986', 'nm0753382', 'nm0359880', 'nm0950000', 'nm0719131', 'nm0782900', 'nm4312760', 'nm1229323', 'nm0455839', 'nm0198014', 'nm0320784', 'nm0956022', 'nm1150959', 'nm0515101', 'nm0324013', 'nm0264236', 'nm0001901', 'nm0006959', 'nm0645516', 'nm1141070', 'nm0697935', 'nm0997841', 'nm0761522', 'nm0847294', 'nm3009132', 'nm5643041', 'nm1226069', 'nm0004056', 'nm1061623', 'nm1371524', 'nm2350892', 'nm0192478', 'nm0060347', 'nm1160071', 'nm3268751', 'nm4562662', 'nm0190859', 'nm0124112', 'nm5050824', 'nm9511013', 'nm0720000', 'nm0919012', 'nm2999308', 'nm0743555', 'nm0367431', 'nm0003080', 'nm3228270', 'nm0068385', 'nm0322515', 'nm0464123', 'nm0517597', 'nm1130275', 'nm3231469', 'nm0325175', 'nm0463596', 'nm1165576', 'nm5449600', 'nm0039468', 'nm0469823', 'nm0751481', 'nm0584485', 'nm6497799', 'nm0334353', 'nm0004111', 'nm0625379', 'nm8734870', 'nm0325776', 'nm0075644', 'nm2905562', 'nm1097114', 'nm0030735', 'nm0287836', 'nm0807421', 'nm0243659', 'nm2918981', 'nm3893748', 'nm0932992', 'nm0831457', 'nm0911642', 'nm0323670', 'nm3728575', 'nm0003116', 'nm0229544', 'nm0132257', 'nm1427994', 'nm11065997', 'nm2123102', 'nm0765761', 'nm1317966', 'nm0777593', 'nm0047028', 'nm7920254', 'nm3159714', 'nm8664327', 'nm0814734', 'nm0824882', 'nm0814716', 'nm0103855', 'nm0525782', 'nm0000797', 'nm0194274', 'nm0614682', 'nm12319791', 'nm0437802', 'nm0087503', 'nm3732772', 'nm5956405', 'nm0579580', 'nm1050748', 'nm2928920', 'nm0111735', 'nm3881964', 'nm0436284', 'nm1248193', 'nm0782947', 'nm0543032', 'nm0714940', 'nm0274659', 'nm1760744', 'nm2057169', 'nm0716814', 'nm7958316', 'nm0672492', 'nm0531751', 'nm1456816', 'nm0290371', 'nm3771227', 'nm0131969', 'nm2253160', 'nm1169248', 'nm0700335', 'nm0015328', 'nm0000698', 'nm12904118', 'nm0210701'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m model.fit(X_train, y_train)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Generate Predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m y_val_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m y_test_pred = model.predict(X_test)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Save predictions in required format (no headers, single column)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1101\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1099\u001b[39m     routed_params = \u001b[38;5;28mself\u001b[39m._get_empty_routing()\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m Xs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_output(Xs)\n\u001b[32m   1110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[32m   1111\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:910\u001b[39m, in \u001b[36mColumnTransformer._call_func_on_transformers\u001b[39m\u001b[34m(self, X, y, func, column_as_labels, routed_params)\u001b[39m\n\u001b[32m    898\u001b[39m             extra_args = {}\n\u001b[32m    899\u001b[39m         jobs.append(\n\u001b[32m    900\u001b[39m             delayed(func)(\n\u001b[32m    901\u001b[39m                 transformer=clone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[32m   (...)\u001b[39m\u001b[32m    907\u001b[39m             )\n\u001b[32m    908\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m910\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\pipeline.py:1531\u001b[39m, in \u001b[36m_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, params)\u001b[39m\n\u001b[32m   1509\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_transform_one\u001b[39m(transformer, X, y, weight, params=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[32m   1511\u001b[39m \n\u001b[32m   1512\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1529\u001b[39m \u001b[33;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[32m   1530\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1531\u001b[39m     res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1532\u001b[39m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[32m   1533\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1597\u001b[39m, in \u001b[36mOrdinalEncoder.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1583\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1584\u001b[39m \u001b[33;03mTransform X to ordinal codes.\u001b[39;00m\n\u001b[32m   1585\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1594\u001b[39m \u001b[33;03m    Transformed input.\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1596\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcategories_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m X_int, X_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_category_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_missing_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1603\u001b[39m X_trans = X_int.astype(\u001b[38;5;28mself\u001b[39m.dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1605\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cat_idx, missing_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._missing_indices.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:218\u001b[39m, in \u001b[36m_BaseEncoder._transform\u001b[39m\u001b[34m(self, X, handle_unknown, ensure_all_finite, warn_on_unknown, ignore_category_indices)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown == \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    214\u001b[39m     msg = (\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m during transform\u001b[39m\u001b[33m\"\u001b[39m.format(diff, i)\n\u001b[32m    217\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[31mValueError\u001b[39m: Found unknown categories ['nm0760196', 'nm0770234', 'nm2493041', 'nm0056030', 'nm0827854', 'nm0475852', 'nm0186091', 'nm0001692', 'nm0797869', 'nm0885108', 'nm0000578', 'nm0272568', 'nm1957032', 'nm0242757', 'nm1260607', 'nm0464180', 'nm2797828', 'nm1223294', 'nm0605266', 'nm0587518', 'nm1624755', 'nm0473329', 'nm1061993', 'nm1000159', 'nm0569891', 'nm2669989', 'nm3306459', 'nm5107883', 'nm6016411', 'nm0431172', 'nm0079387', 'nm1811747', 'nm0718469', 'nm0648740', 'nm0110564', 'nm0533666', 'nm0998110', 'nm0639858', 'nm0423646', 'nm1048540', 'nm8381668', 'nm0522028', 'nm3201804', 'nm4033941', 'nm4980768', 'nm0941262', 'nm1242277', 'nm3155648', 'nm0005197', 'nm0294457', 'nm0801885', 'nm1868917', 'nm1473037', 'nm1936947', 'nm1361273', 'nm1804614', 'nm0762517', 'nm1970598', 'nm0103971', 'nm0508998', 'nm2655364', 'nm0526019', 'nm0446819', 'nm0001774', 'nm0672769', 'nm2258148', 'nm0899121', 'nm0075263', 'nm0394233', 'nm0411259', 'nm3052903', 'nm6174515', 'nm4909815', 'nm3184677', 'nm2746751', 'nm0438089', 'nm0442646', 'nm0648464', 'nm0002225', 'nm1637832', 'nm0495898', 'nm0960253', 'nm0815044', 'nm0607422', 'nm0213726', 'nm1025280', 'nm5068576', 'nm3226283', 'nm2751807', 'nm0112409', 'nm2165987', 'nm0769874', 'nm1427149', 'nm0310597', 'nm1207166', 'nm0453579', 'nm7978876', 'nm0440913', 'nm0744107', 'nm0043214', 'nm0395105', 'nm0688361', 'nm3407375', 'nm0934863', 'nm0744985', 'nm0084114', 'nm1240647', 'nm0542752', 'nm0924429', 'nm0462277', 'nm0845618', 'nm0289800', 'nm0330456', 'nm0389514', 'nm1927689', 'nm0449868', 'nm0653768', 'nm0453115', 'nm0905592', 'nm0586841', 'nm0100522', 'nm0286790', 'nm0699095', 'nm2988421', 'nm4171068', 'nm0602452', 'nm2473806', 'nm1912285', 'nm0660453', 'nm0868871', 'nm0000045', 'nm6512883', 'nm0040575', 'nm1738698', 'nm2269581', 'nm3818919', 'nm1583960', 'nm0373154', 'nm0115269', 'nm4335588', 'nm0013972', 'nm0950620', 'nm1491597', 'nm10470070', 'nm0712444', 'nm3147478', 'nm0909429', 'nm0074723', 'nm1028575', 'nm8784283', 'nm0518363', 'nm0001408', 'nm0732106', 'nm0774143', 'nm0698781', 'nm0076779', 'nm0925482', 'nm0491522', 'nm0775097', 'nm1733624', 'nm0780643', 'nm0559564', 'nm1497931', 'nm0522604', 'nm1630654', 'nm0499215', 'nm7355036', 'nm0511393', 'nm1644484', 'nm8399396', 'nm1104118', 'nm0807312', 'nm1009775', 'nm0000104', 'nm0001471', 'nm0106928', 'nm0381450', 'nm0491708', 'nm0665163', 'nm0425894', 'nm0381478', 'nm1154184', 'nm4756103', 'nm0236226', 'nm1466818', 'nm1913201', 'nm0379797', 'nm1146668', 'nm0164999', 'nm0366004', 'nm0725345', 'nm0629580', 'nm0006639', 'nm0869857', 'nm0953216', 'nm1215448', 'nm4183075', 'nm1190532', 'nm2171755', 'nm0466349', 'nm1154600', 'nm0281869', 'nm1783265', 'nm0672704', 'nm1079001', 'nm0071611', 'nm5832876', 'nm0690677', 'nm0618405', 'nm0641007', 'nm0397565', 'nm0826814', 'nm1317258', 'nm0210218', 'nm0523154', 'nm0427037', 'nm2719270', 'nm0364252', 'nm0945412', 'nm2264935', 'nm1027891', 'nm0786919', 'nm1062423', 'nm0014960', 'nm2403746', 'nm0215085', 'nm0217803', 'nm0089176', 'nm0504641', 'nm6304969', 'nm0064593', 'nm1206844', 'nm0056527', 'nm0359081', 'nm0013288', 'nm1119099', 'nm0139111', 'nm0836715', 'nm0350455', 'nm1577772', 'nm0562266', 'nm0258418', 'nm0317411', 'nm3640095', 'nm2692858', 'nm0504654', 'nm2165634', 'nm7618090', 'nm1145617', 'nm0575389', 'nm1279573', 'nm1293863', 'nm0382779', 'nm1077277', 'nm1561205', 'nm0892400', 'nm0017004', 'nm0906476', 'nm4364444', 'nm2325602', 'nm0406687', 'nm0229694', 'nm0162979', 'nm8011325', 'nm0468882', 'nm1231757', 'nm0839268', 'nm0918041', 'nm0086144', 'nm0580727', 'nm2975950', 'nm0049335', 'nm1362893', 'nm1840195', 'nm0568478', 'nm2348718', 'nm0245361', 'nm0187834', 'nm0769703', 'nm4032740', 'nm1392439', 'nm0954500', 'nm5824384', 'nm1262599', 'nm7916459', 'nm0168892', 'nm3255161', 'nm0779011', 'nm0514816', 'nm0170504', 'nm3616958', 'nm0683578', 'nm0196930', 'nm0233050', 'nm1558526', 'nm0412220', 'nm0009053', 'nm1371451', 'nm1685252', 'nm1204790', 'nm0030612', 'nm0340244', 'nm9214307', 'nm0404606', 'nm0178876', 'nm7552433', 'nm0352524', 'nm5617646', 'nm0663489', 'nm0373612', 'nm0063581', 'nm0916859', 'nm0836708', 'nm0006939', 'nm2395339', 'nm0661919', 'nm2128335', 'nm4298518', 'nm0001490', 'nm0481195', 'nm3936005', 'nm0768959', 'nm1396593', 'nm0773108', 'nm0292134', 'nm0645661', 'nm3192191', 'nm0344144', 'nm6921776', 'nm1902132', 'nm0848992', 'nm1220246', 'nm2085752', 'nm0228013', 'nm1434014', 'nm0915304', 'nm0622288', 'nm1259604', 'nm0861899', 'nm0025399', 'nm0465551', 'nm1026530', 'nm0734466', 'nm0446059', 'nm0770647', 'nm0006853', 'nm0566671', 'nm2141317', 'nm0493615', 'nm0051156', 'nm0262402', 'nm0561938', 'nm0046480', 'nm0009942', 'nm3010986', 'nm0753382', 'nm0359880', 'nm0950000', 'nm0719131', 'nm0782900', 'nm4312760', 'nm1229323', 'nm0455839', 'nm0198014', 'nm0320784', 'nm0956022', 'nm1150959', 'nm0515101', 'nm0324013', 'nm0264236', 'nm0001901', 'nm0006959', 'nm0645516', 'nm1141070', 'nm0697935', 'nm0997841', 'nm0761522', 'nm0847294', 'nm3009132', 'nm5643041', 'nm1226069', 'nm0004056', 'nm1061623', 'nm1371524', 'nm2350892', 'nm0192478', 'nm0060347', 'nm1160071', 'nm3268751', 'nm4562662', 'nm0190859', 'nm0124112', 'nm5050824', 'nm9511013', 'nm0720000', 'nm0919012', 'nm2999308', 'nm0743555', 'nm0367431', 'nm0003080', 'nm3228270', 'nm0068385', 'nm0322515', 'nm0464123', 'nm0517597', 'nm1130275', 'nm3231469', 'nm0325175', 'nm0463596', 'nm1165576', 'nm5449600', 'nm0039468', 'nm0469823', 'nm0751481', 'nm0584485', 'nm6497799', 'nm0334353', 'nm0004111', 'nm0625379', 'nm8734870', 'nm0325776', 'nm0075644', 'nm2905562', 'nm1097114', 'nm0030735', 'nm0287836', 'nm0807421', 'nm0243659', 'nm2918981', 'nm3893748', 'nm0932992', 'nm0831457', 'nm0911642', 'nm0323670', 'nm3728575', 'nm0003116', 'nm0229544', 'nm0132257', 'nm1427994', 'nm11065997', 'nm2123102', 'nm0765761', 'nm1317966', 'nm0777593', 'nm0047028', 'nm7920254', 'nm3159714', 'nm8664327', 'nm0814734', 'nm0824882', 'nm0814716', 'nm0103855', 'nm0525782', 'nm0000797', 'nm0194274', 'nm0614682', 'nm12319791', 'nm0437802', 'nm0087503', 'nm3732772', 'nm5956405', 'nm0579580', 'nm1050748', 'nm2928920', 'nm0111735', 'nm3881964', 'nm0436284', 'nm1248193', 'nm0782947', 'nm0543032', 'nm0714940', 'nm0274659', 'nm1760744', 'nm2057169', 'nm0716814', 'nm7958316', 'nm0672492', 'nm0531751', 'nm1456816', 'nm0290371', 'nm3771227', 'nm0131969', 'nm2253160', 'nm1169248', 'nm0700335', 'nm0015328', 'nm0000698', 'nm12904118', 'nm0210701'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # Define Features & Target\n",
    "# bert_columns = [f\"bert_{i}\" for i in range(df_train[\"distilbert_embedding\"][0].shape[0])]\n",
    "\n",
    "# # Expand BERT embeddings\n",
    "# bert_train = pd.DataFrame(df_train[\"distilbert_embedding\"].tolist(), columns=bert_columns)\n",
    "# bert_val = pd.DataFrame(df_val[\"distilbert_embedding\"].tolist(), columns=bert_columns)\n",
    "# bert_test = pd.DataFrame(df_test[\"distilbert_embedding\"].tolist(), columns=bert_columns)\n",
    "\n",
    "# # Reset index\n",
    "# bert_train.index = df_train.index\n",
    "# bert_val.index = df_val.index\n",
    "# bert_test.index = df_test.index\n",
    "\n",
    "# # Merge expanded embeddings\n",
    "# df_train = pd.concat([df_train, bert_train], axis=1)\n",
    "# df_val = pd.concat([df_val, bert_val], axis=1)\n",
    "# df_test = pd.concat([df_test, bert_test], axis=1)\n",
    "\n",
    "# Define features\n",
    "features = [\"Year\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\", \"word_count\"]\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[\"label\"]\n",
    "X_val = df_val[features]\n",
    "X_test = df_test[features]\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_features = [\"Year\", \"runtimeMinutes\", \"numVotes\"]\n",
    "categorical_features = [\"director_id\", \"writer_id\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OrdinalEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train RandomForest Model\n",
    "model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"ðŸ”¹ Training model on full training data...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate Predictions\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions in required format (no headers, single column)\n",
    "pd.DataFrame(y_val_pred).to_csv(\"submissions/validation_predictions_TitleFeatures.csv\", index=False, header=False)\n",
    "pd.DataFrame(y_test_pred).to_csv(\"submissions/test_predictions_TitleFeatures.csv\", index=False, header=False)\n",
    "\n",
    "print(\"âœ… Predictions saved for submission!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aac51189-75a4-4d83-899a-cb5bbec2b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing features in X_train_small: ['Year', 'runtimeMinutes', 'numVotes', 'director_id', 'writer_id']\n",
      "ðŸ”¹ Training model on 80% of train data...\n",
      "\n",
      "ðŸ“Š Model Evaluation on Train Data Split (80/20)\n",
      "âœ… Accuracy: 0.5496\n",
      "âœ… Precision: 0.5287\n",
      "âœ… Recall: 0.9336\n",
      "âœ… F1 Score: 0.6751\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split df_train into train and validation (80/20 split)\n",
    "df_train_small, df_val_small = train_test_split(df_train, test_size=0.2, random_state=42, stratify=df_train[\"label\"])\n",
    "\n",
    "# # Define Features\n",
    "# bert_columns = [f\"bert_{i}\" for i in range(df_train[\"distilbert_embedding\"][0].shape[0])]\n",
    "\n",
    "# # Expand BERT embeddings\n",
    "# bert_train_small = pd.DataFrame(df_train_small[\"distilbert_embedding\"].tolist(), columns=bert_columns)\n",
    "# bert_val_small = pd.DataFrame(df_val_small[\"distilbert_embedding\"].tolist(), columns=bert_columns)\n",
    "\n",
    "# # Reset index\n",
    "# bert_train_small.index = df_train_small.index\n",
    "# bert_val_small.index = df_val_small.index\n",
    "\n",
    "# # Merge expanded embeddings\n",
    "# df_train_small = pd.concat([df_train_small, bert_train_small], axis=1)\n",
    "# df_val_small = pd.concat([df_val_small, bert_val_small], axis=1)\n",
    "\n",
    "# Define Features & Target\n",
    "features = [\"Year\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\", \"word_count\"]\n",
    "X_train_small = df_train_small[features]\n",
    "y_train_small = df_train_small[\"label\"]\n",
    "X_val_small = df_val_small[features]\n",
    "y_val_small = df_val_small[\"label\"]\n",
    "\n",
    "# âœ… Check if all features exist\n",
    "print(\"Existing features in X_train_small:\", X_train_small.columns.tolist())\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_features = [\"Year\", \"runtimeMinutes\", \"numVotes\"]\n",
    "categorical_features = [\"director_id\", \"writer_id\"]\n",
    "\n",
    "# âœ… Fix: Handle unknown categories in `OrdinalEncoder`\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), categorical_features)  # âœ… Fix applied\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train RandomForest Model\n",
    "model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"ðŸ”¹ Training model on 80% of train data...\")\n",
    "model.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Generate Predictions on Validation Set\n",
    "y_val_pred = model.predict(X_val_small)\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "accuracy = accuracy_score(y_val_small, y_val_pred)\n",
    "precision = precision_score(y_val_small, y_val_pred, average=\"binary\")\n",
    "recall = recall_score(y_val_small, y_val_pred, average=\"binary\")\n",
    "f1 = f1_score(y_val_small, y_val_pred, average=\"binary\")\n",
    "\n",
    "# Print Results\n",
    "print(\"\\nðŸ“Š Model Evaluation on Train Data Split (80/20)\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb050ac3-002c-4b2e-920c-4891058a7cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
