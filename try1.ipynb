{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c195d0ab-b1eb-46eb-95a5-57cea85bb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734438c9-8ab9-4298-b3ae-822bbbad1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa09ba3b-438a-4f68-82bb-25ba9fa34344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>tt0011439</td>\n",
       "      <td>The Mark of Zorro</td>\n",
       "      <td>The Mark of Zorro</td>\n",
       "      <td>1920</td>\n",
       "      <td>\\N</td>\n",
       "      <td>79</td>\n",
       "      <td>2439.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>tt0012532</td>\n",
       "      <td>Ớrpháns ớf thé Stớrm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1921</td>\n",
       "      <td>\\N</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>tt0013933</td>\n",
       "      <td>The Faithful Heart</td>\n",
       "      <td>Coeur fidèle</td>\n",
       "      <td>1923</td>\n",
       "      <td>\\N</td>\n",
       "      <td>87</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>tt0015400</td>\n",
       "      <td>The Thief of Bagdad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1924</td>\n",
       "      <td>\\N</td>\n",
       "      <td>155</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>tt0015842</td>\n",
       "      <td>The Joyless Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1925</td>\n",
       "      <td>\\N</td>\n",
       "      <td>125</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     tconst          primaryTitle      originalTitle startYear  \\\n",
       "0           5  tt0011439     The Mark of Zorro  The Mark of Zorro      1920   \n",
       "1          10  tt0012532  Ớrpháns ớf thé Stớrm                NaN      1921   \n",
       "2          13  tt0013933    The Faithful Heart       Coeur fidèle      1923   \n",
       "3          31  tt0015400   The Thief of Bagdad                NaN      1924   \n",
       "4          33  tt0015842    The Joyless Street                NaN      1925   \n",
       "\n",
       "  endYear runtimeMinutes  numVotes  label  \n",
       "0      \\N             79    2439.0   True  \n",
       "1      \\N            150       NaN   True  \n",
       "2      \\N             87    1252.0   True  \n",
       "3      \\N            155    6001.0   True  \n",
       "4      \\N            125    1554.0   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd376dc-62e5-43eb-ba3a-990f01d6b804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          int64\n",
       "tconst             object\n",
       "primaryTitle       object\n",
       "originalTitle      object\n",
       "startYear          object\n",
       "endYear            object\n",
       "runtimeMinutes     object\n",
       "numVotes          float64\n",
       "label                bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95831fac-1292-4581-9ab0-610c975b35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column before imputation:\n",
      "Unnamed: 0          0\n",
      "tconst              0\n",
      "primaryTitle        0\n",
      "startYear         105\n",
      "endYear           846\n",
      "runtimeMinutes      1\n",
      "numVotes          114\n",
      "label               0\n",
      "dtype: int64\n",
      "Missing values per column after imputation:\n",
      "Unnamed: 0          0\n",
      "tconst              0\n",
      "primaryTitle        0\n",
      "startYear           0\n",
      "endYear           846\n",
      "runtimeMinutes      0\n",
      "numVotes            0\n",
      "label               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>tt0011439</td>\n",
       "      <td>The Mark of Zorro</td>\n",
       "      <td>1920</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>79</td>\n",
       "      <td>2439.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>tt0012532</td>\n",
       "      <td>Ớrpháns ớf thé Stớrm</td>\n",
       "      <td>1921</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>tt0013933</td>\n",
       "      <td>The Faithful Heart</td>\n",
       "      <td>1923</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>87</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>tt0015400</td>\n",
       "      <td>The Thief of Bagdad</td>\n",
       "      <td>1924</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>155</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>tt0015842</td>\n",
       "      <td>The Joyless Street</td>\n",
       "      <td>1925</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>125</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     tconst          primaryTitle  startYear  endYear  \\\n",
       "0           5  tt0011439     The Mark of Zorro       1920     <NA>   \n",
       "1          10  tt0012532  Ớrpháns ớf thé Stớrm       1921     <NA>   \n",
       "2          13  tt0013933    The Faithful Heart       1923     <NA>   \n",
       "3          31  tt0015400   The Thief of Bagdad       1924     <NA>   \n",
       "4          33  tt0015842    The Joyless Street       1925     <NA>   \n",
       "\n",
       "   runtimeMinutes  numVotes  label  \n",
       "0              79    2439.0   True  \n",
       "1             150       0.0   True  \n",
       "2              87    1252.0   True  \n",
       "3             155    6001.0   True  \n",
       "4             125    1554.0   True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Drop 'originalTitle' column (if required)\n",
    "df.drop(columns=[\"originalTitle\"], inplace=True)\n",
    "\n",
    "# Step 2: Convert relevant columns to integers, handling errors\n",
    "cols_to_convert = [\"startYear\", \"endYear\", \"runtimeMinutes\"]\n",
    "for col in cols_to_convert:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")  # Keeps NaNs as <NA>\n",
    "\n",
    "# Step 3: Check for missing values\n",
    "print(\"Missing values per column before imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 4: Calculate the average difference between startYear and endYear\n",
    "valid_years = df.dropna(subset=[\"startYear\", \"endYear\"])  # Keep only complete cases\n",
    "avg_duration = (valid_years[\"endYear\"] - valid_years[\"startYear\"]).mean()\n",
    "\n",
    "# Step 5: Fill missing endYear values by adding the average duration to startYear\n",
    "df.loc[df[\"endYear\"].isna(), \"endYear\"] = df[\"startYear\"] + avg_duration\n",
    "df[\"endYear\"] = df[\"endYear\"].round().astype(\"Int64\")  # Convert to integer\n",
    "\n",
    "# Step 6: Handle other missing values\n",
    "df.loc[:, \"startYear\"] = df[\"startYear\"].fillna(df[\"startYear\"].median())\n",
    "df.loc[:, \"runtimeMinutes\"] = df[\"runtimeMinutes\"].fillna(df[\"runtimeMinutes\"].median())\n",
    "df.loc[:, \"numVotes\"] = df[\"numVotes\"].fillna(0)\n",
    "\n",
    "# Step 7: Display cleaned data\n",
    "print(\"Missing values per column after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e36f268-6082-49ff-9ab5-ff3721bd21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column after cleanup:\n",
      "Unnamed: 0        0\n",
      "tconst            0\n",
      "primaryTitle      0\n",
      "startYear         0\n",
      "runtimeMinutes    0\n",
      "numVotes          0\n",
      "label             0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>tt0011439</td>\n",
       "      <td>The Mark of Zorro</td>\n",
       "      <td>1920</td>\n",
       "      <td>79</td>\n",
       "      <td>2439.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>tt0012532</td>\n",
       "      <td>Ớrpháns ớf thé Stớrm</td>\n",
       "      <td>1921</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>tt0013933</td>\n",
       "      <td>The Faithful Heart</td>\n",
       "      <td>1923</td>\n",
       "      <td>87</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>tt0015400</td>\n",
       "      <td>The Thief of Bagdad</td>\n",
       "      <td>1924</td>\n",
       "      <td>155</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>tt0015842</td>\n",
       "      <td>The Joyless Street</td>\n",
       "      <td>1925</td>\n",
       "      <td>125</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     tconst          primaryTitle  startYear  runtimeMinutes  \\\n",
       "0           5  tt0011439     The Mark of Zorro       1920              79   \n",
       "1          10  tt0012532  Ớrpháns ớf thé Stớrm       1921             150   \n",
       "2          13  tt0013933    The Faithful Heart       1923              87   \n",
       "3          31  tt0015400   The Thief of Bagdad       1924             155   \n",
       "4          33  tt0015842    The Joyless Street       1925             125   \n",
       "\n",
       "   numVotes  label  \n",
       "0    2439.0   True  \n",
       "1       0.0   True  \n",
       "2    1252.0   True  \n",
       "3    6001.0   True  \n",
       "4    1554.0   True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-3.csv\")\n",
    "\n",
    "# Step 1: Drop 'originalTitle' and 'endYear' columns\n",
    "df.drop(columns=[\"originalTitle\", \"endYear\"], inplace=True)\n",
    "\n",
    "# Step 2: Convert relevant columns to integers, handling errors\n",
    "cols_to_convert = [\"startYear\", \"runtimeMinutes\"]\n",
    "for col in cols_to_convert:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")  # Keeps NaNs as <NA>\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "df[\"startYear\"] = df[\"startYear\"].fillna(df[\"startYear\"].median()).astype(\"Int64\")\n",
    "df[\"runtimeMinutes\"] = df[\"runtimeMinutes\"].fillna(df[\"runtimeMinutes\"].median()).astype(\"Int64\")\n",
    "df[\"numVotes\"] = df[\"numVotes\"].fillna(0)\n",
    "\n",
    "# Step 4: Save the cleaned dataset\n",
    "df.to_csv(\"cleaned_file.csv\", index=False)\n",
    "\n",
    "# Step 5: Display cleaned data summary\n",
    "print(\"Missing values per column after cleanup:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bbb77c-9ca5-4789-8f3e-0dac3fe79617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7016\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.69      0.80      0.74       103\n",
      "        True       0.71      0.59      0.65        88\n",
      "\n",
      "    accuracy                           0.70       191\n",
      "   macro avg       0.70      0.69      0.69       191\n",
      "weighted avg       0.70      0.70      0.70       191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"cleaned_file.csv\")\n",
    "\n",
    "# Step 1: Select features and target variable\n",
    "features = [\"startYear\", \"runtimeMinutes\", \"numVotes\"]  # Numerical features\n",
    "X = df[features]\n",
    "y = df[\"label\"]  # Target variable\n",
    "\n",
    "# Step 2: Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Normalize numerical features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train a simple Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")  # Prints accuracy score\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03ebb07-4b4e-4d7d-a82a-55c8124c4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directors JSON Columns: Index(['movie', 'director'], dtype='object')\n",
      "Writers JSON Columns: Index(['movie', 'writer'], dtype='object')\n",
      "       movie   director\n",
      "0  tt0003740  nm0665163\n",
      "1  tt0008663  nm0803705\n",
      "2  tt0009369  nm0428059\n",
      "3  tt0009369  nm0949648\n",
      "4  tt0010307  nm0304098\n",
      "       movie     writer\n",
      "0  tt0003740  nm0195339\n",
      "1  tt0003740  nm0515385\n",
      "2  tt0003740  nm0665163\n",
      "3  tt0003740  nm0758215\n",
      "4  tt0008663  nm0406585\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON files\n",
    "df_directors = pd.read_json(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/directing.json\")\n",
    "df_writers = pd.read_json(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/writing.json\")\n",
    "\n",
    "# Show column names\n",
    "print(\"Directors JSON Columns:\", df_directors.columns)\n",
    "print(\"Writers JSON Columns:\", df_writers.columns)\n",
    "\n",
    "# Show first few rows\n",
    "print(df_directors.head())\n",
    "print(df_writers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8e4780-80e3-4d45-ab5c-68097df201b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merge successful! First few rows:\n",
      "   Unnamed: 0     tconst                   primaryTitle  \\\n",
      "0           5  tt0011439              The Mark of Zorro   \n",
      "1          10  tt0012532           Ớrpháns ớf thé Stớrm   \n",
      "2          13  tt0013933             The Faithful Heart   \n",
      "3          31  tt0015400            The Thief of Bagdad   \n",
      "4          43  tt0016641  Ben-Hur: A Tale of the Christ   \n",
      "\n",
      "                   originalTitle startYear endYear runtimeMinutes  numVotes  \\\n",
      "0              The Mark of Zorro      1920      \\N             79    2439.0   \n",
      "1                           None      1921      \\N            150       NaN   \n",
      "2                   Coeur fidèle      1923      \\N             87    1252.0   \n",
      "3                           None      1924      \\N            155    6001.0   \n",
      "4  Ben-Hur: A Tale of the Christ      1925      \\N            143    7539.0   \n",
      "\n",
      "   label director_id  writer_id  \n",
      "0   True   nm0629243  nm1491939  \n",
      "1   True   nm0000428  nm0000428  \n",
      "2   True   nm0258477  nm0258506  \n",
      "3   True   nm0909825  nm0640884  \n",
      "4   True   nm0629243  nm0129721  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df_movies = pd.read_csv(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/train-3.csv\")\n",
    "df_directors = pd.read_json(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/directing.json\")\n",
    "df_writers = pd.read_json(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/BD/BigData-Group10/imdb/writing.json\")\n",
    "\n",
    "# Rename 'movie' column to 'tconst' to match movies dataset\n",
    "df_directors.rename(columns={\"movie\": \"tconst\", \"director\": \"director_id\"}, inplace=True)\n",
    "df_writers.rename(columns={\"movie\": \"tconst\", \"writer\": \"writer_id\"}, inplace=True)\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Register Pandas DataFrames as DuckDB tables\n",
    "con.register(\"movies\", df_movies)\n",
    "con.register(\"directors\", df_directors)\n",
    "con.register(\"writers\", df_writers)\n",
    "\n",
    "# Perform the join\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    movies.*, \n",
    "    directors.director_id, \n",
    "    writers.writer_id\n",
    "FROM movies\n",
    "LEFT JOIN directors ON movies.tconst = directors.tconst\n",
    "LEFT JOIN writers ON movies.tconst = writers.tconst\n",
    "\"\"\"\n",
    "\n",
    "# Run query and get final merged dataset\n",
    "df_merged = con.execute(query).fetchdf()\n",
    "\n",
    "# Save to CSV\n",
    "df_merged.to_csv(\"merged_data.csv\", index=False)\n",
    "\n",
    "# Close DuckDB connection\n",
    "con.close()\n",
    "\n",
    "print(\"✅ Merge successful! First few rows:\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ae8552-5678-46a2-98b2-9ab069df74a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_11184\\3659705994.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"director_id\"].fillna(\"unknown\", inplace=True)  # Placeholder for missing directors\n",
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_11184\\3659705994.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"writer_id\"].fillna(\"unknown\", inplace=True)  # Placeholder for missing writers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     42\u001b[39m model = Pipeline([\n\u001b[32m     43\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m\"\u001b[39m, preprocessor),\n\u001b[32m     44\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m, LogisticRegression(max_iter=\u001b[32m1000\u001b[39m))\n\u001b[32m     45\u001b[39m ])\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Step 8: Train the Model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Step 9: Make Predictions\u001b[39;00m\n\u001b[32m     51\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1014\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(array):\n\u001b[32m   1013\u001b[39m     _ensure_no_complex_data(array)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     array = \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m array.ndim < \u001b[32m2\u001b[39m:\n\u001b[32m   1025\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1026\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D input, got input with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1027\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1028\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:649\u001b[39m, in \u001b[36m_ensure_sparse_format\u001b[39m\u001b[34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[39m\n\u001b[32m    644\u001b[39m         warnings.warn(\n\u001b[32m    645\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt check \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_container.format\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sparse matrix for nan or inf.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    646\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    647\u001b[39m         )\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m            \u001b[49m\u001b[43msparse_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# TODO: Remove when the minimum version of SciPy supported is 1.12\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# With SciPy sparse arrays, conversion from DIA format to COO, CSR, or BSR\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# triggers the use of `np.int64` indices even if the data is such that it could\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# algorithms support large indices, the following code downcasts to `np.int32`\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;66;03m# indices when it's safe to do so.\u001b[39;00m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m changed_format:\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# accept_sparse is specified to a specific format and a conversion occurred\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\UVA\\Vakken met code\\BD\\BigData-Group10\\BD_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load merged dataset\n",
    "df = pd.read_csv(\"merged_data.csv\")\n",
    "\n",
    "# Step 1: Replace `\\N` with NaN\n",
    "df.replace(\"\\\\N\", np.nan, inplace=True)\n",
    "\n",
    "# Step 2: Handle missing values in director_id and writer_id\n",
    "df[\"director_id\"].fillna(\"unknown\", inplace=True)  # Placeholder for missing directors\n",
    "df[\"writer_id\"].fillna(\"unknown\", inplace=True)  # Placeholder for missing writers\n",
    "\n",
    "# Step 3: Convert relevant columns to numeric\n",
    "numeric_features = [\"startYear\", \"runtimeMinutes\", \"numVotes\"]\n",
    "df[numeric_features] = df[numeric_features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Step 4: Select features and target\n",
    "features = [\"startYear\", \"runtimeMinutes\", \"numVotes\", \"director_id\", \"writer_id\"]\n",
    "X = df[features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Step 5: Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Preprocessing (Standardize numeric features + One-Hot Encode categorical)\n",
    "categorical_features = [\"director_id\", \"writer_id\"]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),  # Standardize numerical data\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)  # Encode categorical data\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 7: Create Pipeline for Preprocessing + Logistic Regression\n",
    "model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Step 8: Train the Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23273e3-8e13-4917-91ad-b32753e4e1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53653259-8198-4429-8a66-95a02b161098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583451f-71c6-4cf0-8847-d2040e584707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
